{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import requests\n",
    "from tqdm import trange\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib\n",
    "from random import randint\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "from skimage.transform import rescale, resize\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate, Cropping2D, Reshape, BatchNormalization\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "import h5py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, roc_auc_score\n",
    "\n",
    "from create_model import createResNet50, CreateKaggleModel\n",
    "\n",
    "import stainNorm_Reinhard as stainNorm\n",
    "\n",
    "\n",
    "# this part is needed if you run the notebook on Cartesius with multiple cores\n",
    "#n_cores = 32\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=n_cores-1, inter_op_parallelism_threads=1, allow_soft_placement=True)\n",
    "#session = tf.Session(config=config)\n",
    "#K.set_session(session)\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = str(n_cores-1)\n",
    "#os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "#os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "#os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape = [2,3], name = 'a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape = [3,2], name = 'b')\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement = True))\n",
    "print(sess.run(c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, data_dir_x, data_dir_y = None, use_percentage = 1.0):\n",
    "        self.use_percentage = use_percentage\n",
    "        self.data_dir_x = data_dir_x\n",
    "        self.data_dir_y = data_dir_y\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        with h5py.File(self.data_dir_x, 'r') as hdf:\n",
    "            return (int)(len(hdf['x'])*self.use_percentage)\n",
    "    \n",
    "    def get_data(self):\n",
    "        with h5py.File(self.data_dir_x, 'r') as hdf:\n",
    "            return np.array( list(hdf['x']))/255\n",
    "        \n",
    "    def get_lbls(self):\n",
    "        with h5py.File(self.data_dir_y, 'r') as hdf:\n",
    "            data = list(hdf['y'])\n",
    "            return np.reshape(data,(len(data))) \n",
    "        \n",
    "    \n",
    "    def show_image(self, i):\n",
    "        with h5py.File(self.data_dir_x, 'r') as hdf:\n",
    "            data = list(hdf['x'][i])\n",
    "            img = np.array(data)\n",
    "            plt.rcParams['figure.figsize'] = (3, 3)\n",
    "            plt.imshow(img)\n",
    "            plt.title('RGB image')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentor:\n",
    "    def __inint__(self):\n",
    "        print(\"init\")\n",
    "        \n",
    "    def rotate(self, image):\n",
    "        #rotation of 10 degrees cc\n",
    "        rows,cols,channels = image.shape\n",
    "        deg = np.random.randint(360)\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2),deg,1)\n",
    "        image = cv2.warpAffine(image,M,(cols,rows))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, dataset, augmentor = None):\n",
    "        self.dataset = dataset\n",
    "        self.dataset_length = self.dataset.get_lenght()\n",
    "        self.augmentor = augmentor\n",
    "       \n",
    "        self.index = []\n",
    "        self.batch = []\n",
    "        self.i = 0\n",
    "        \n",
    "        \n",
    "    def create_batch(self, batch_size):\n",
    "        x_data = np.array([])\n",
    "        y_data = np.array([])\n",
    "        \n",
    "        if(self.i+batch_size > self.dataset_length):\n",
    "            self.i = 0\n",
    "      \n",
    "        with h5py.File(self.dataset.data_dir_x, 'r') as hdf:\n",
    "            data = np.array(list(hdf['x'][self.i:self.i+batch_size]))/255\n",
    "            \n",
    "            if(self.augmentor != None):\n",
    "                #rotates ever image with an random angle of 0-360deg\n",
    "                for i in range(len(data)):\n",
    "                    data[i] = self.augmentor.rotate(data[i])\n",
    "            \n",
    "            x_data = data\n",
    "        \n",
    "        with h5py.File(self.dataset.data_dir_y, 'r') as hdf:\n",
    "            data = list(hdf['y'][self.i:self.i+batch_size])\n",
    "            data = np.reshape(data,(batch_size,1))\n",
    "            data = to_categorical(data, num_classes=2)\n",
    "            y_data = np.array(data) \n",
    "            \n",
    "        self.i += batch_size\n",
    "            \n",
    "        return x_data, y_data\n",
    "    \n",
    "\n",
    "    def create_batch_sampling(self, batch_size):\n",
    "        #init empty batch\n",
    "        x_data = np.zeros( (batch_size, 96,96,3) )\n",
    "        y_data = np.zeros( (batch_size, 2) )\n",
    "        \n",
    "        #copy the patches which have to be run again\n",
    "        for i in range(len(self.index)):\n",
    "            x_data[i] = self.batch[0][self.index[i]] \n",
    "            y_data[i] = self.batch[1][self.index[i]]\n",
    "            \n",
    "        #fill the rest of the batch with new patches\n",
    "        rest = np.arange(len(self.index), batch_size)\n",
    "        for i in rest:\n",
    "            #get random position\n",
    "            random_pos = np.random.randint(0,self.dataset_length)\n",
    "            #open x file\n",
    "            with h5py.File(self.dataset.data_dir_x, 'r') as hdf:\n",
    "                data = np.array(list(hdf['x'][random_pos]))/255\n",
    "                if(self.augmentor != None):\n",
    "                    #rotates every image with an random angle of 0-360deg\n",
    "                    data = self.augmentor.rotate(data)\n",
    "                \n",
    "                x_data[i] = data\n",
    "\n",
    "            # open y file\n",
    "            with h5py.File(self.dataset.data_dir_y, 'r') as hdf:\n",
    "                data = list(hdf['y'][i])\n",
    "                data = np.reshape(data,(1))\n",
    "                data = to_categorical(data, num_classes=2)\n",
    "                y_data[i] = data\n",
    "\n",
    "        self.batch = [x_data, y_data]\n",
    "            \n",
    "        return x_data, y_data\n",
    "    \n",
    "        \n",
    "    def get_generator(self, batch_size):\n",
    "        '''returns a generator that will yield batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_batch(batch_size)\n",
    "            \n",
    "    def get_sampling_generator(self, batch_size):\n",
    "        '''returns a generator that will yield batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_batch_sampling(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, validation_data, saving_dir, model_name, batch_creator, sampling_threshold, sampling):\n",
    "        #validation set\n",
    "        self.validation_dataset = validation_data\n",
    "        #traing batch generator\n",
    "        self.batch_creator = batch_creator\n",
    "        #threshold to for retraining a given sample\n",
    "        self.sampling_threshold = sampling_threshold\n",
    "        self.sampling = sampling\n",
    "        self.time = time.time()\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        #get the validation data set\n",
    "        self.val_imgs = self.validation_dataset.get_data()\n",
    "        self.val_lbls = self.validation_dataset.get_lbls()\n",
    "        \n",
    "        self.Sensitivity = []\n",
    "        self.Specificity = []\n",
    "        self.auc = 0\n",
    "        self.aucs = []\n",
    "        \n",
    "        self.model_filename = os.path.join(saving_dir, model_name + '.h5')\n",
    "        \n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.best_validation_acc = 0\n",
    "        self.best_model = None\n",
    "        self.predictions = None\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if self.sampling:\n",
    "            \n",
    "            y_pred = self.model.predict(self.batch_creator.batch[0])\n",
    "            y_true = self.batch_creator.batch[1][:,1]\n",
    "            y_pred_argmax = np.argmax(y_pred, axis = 1)\n",
    "            y_pred_max = np.max(y_pred, axis = 1)\n",
    "\n",
    "            index = []\n",
    "            for i in range(len(y_true)):\n",
    "                # if the prediction is wrong add the retrain the sample\n",
    "                if y_true[i] != y_pred_argmax[i]:\n",
    "                    index.append(i)\n",
    "                #else if the prediction is correct but below the threshold retrain the sample\n",
    "                elif y_pred_max[i] < self.sampling_threshold:\n",
    "                    index.append(i)\n",
    "            self.batch_creator.index = index\n",
    "    \n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        acc = self.validate()\n",
    "        self.accuracies.append([len(self.losses), acc])\n",
    "        if acc > self.best_validation_acc:\n",
    "            self.best_validation_acc = acc\n",
    "            self.model.save(self.model_filename) # save best model to disk\n",
    "            print('best model saved as {}'.format(self.model_filename))\n",
    "        self.plot()   \n",
    "        \n",
    "        t = time.time() - self.time\n",
    "        hours = int(t/3600)\n",
    "        t = t - hours*3600\n",
    "        minutes = int(t/60)\n",
    "        t = t - minutes*60\n",
    "        seconds = t\n",
    "    \n",
    "        #print to logger file\n",
    "        with open(\"./logs/current_status_{}.txt\".format(self.model_name), \"a\") as myfile:\n",
    "            myfile.write(str(epoch) + ' | ' + str(logs) + ' | val_acc = ' + str(acc)+' | auc = '+str(self.auc) \n",
    "                         +' | time = '+str(hours)+':'+str(minutes)+':'+str(seconds)+'\\n')\n",
    "        \n",
    "    \n",
    "    def validate(self):\n",
    "        #predict validation set\n",
    "        pred_lbls = self.model.predict(self.val_imgs)\n",
    "        predicted_lbls = np.argmax(pred_lbls, axis=1)\n",
    "        \n",
    "        self.Specificity, self.Sensitivity, thresholds = roc_curve(self.val_lbls, pred_lbls[:,1])\n",
    "        self.auc = roc_auc_score(self.val_lbls, pred_lbls[:,1])\n",
    "        self.aucs.append(self.auc)\n",
    "                    \n",
    "        return compute_accuracy(self.val_lbls, predicted_lbls)\n",
    "\n",
    "    \n",
    "    def plot(self):\n",
    "        clear_output()\n",
    "        N = len(self.losses)\n",
    "        plt.figure(figsize=(50, 10))\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.plot(range(0, N), self.losses); plt.title('losses')\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.plot(np.array(self.accuracies)); plt.title('accuracies')\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.plot(np.array(self.aucs)); plt.title('AUC')\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.plot(np.array(self.Specificity), np.array(self.Sensitivity)); \n",
    "        plt.title(' AUC = {}'.format(self.auc));\n",
    "        plt.xlabel('1 - Specificity');\n",
    "        plt.ylabel('Sensitivity')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data dirs\n",
    "data_dir = './data/'\n",
    "data_dir_norm = './data/normalized/'\n",
    "\n",
    "#train_dir_x = os.path.join(data_dir_norm, 'x_train.h5')\n",
    "#train_dir_y = os.path.join(data_dir, 'camelyonpatch_level_2_split_train_y.h5')\n",
    "#valid_dir_x = os.path.join(data_dir_norm, 'x_valid.h5')\n",
    "#valid_dir_y = os.path.join(data_dir, 'camelyonpatch_level_2_split_valid_y.h5')\n",
    "#test_dir_x = os.path.join(data_dir_norm, 'x_test.h5')\n",
    "\n",
    "train_dir_x = os.path.join(data_dir, 'camelyonpatch_level_2_split_train_x.h5')\n",
    "train_dir_y = os.path.join(data_dir, 'camelyonpatch_level_2_split_train_y.h5')\n",
    "valid_dir_x = os.path.join(data_dir, 'camelyonpatch_level_2_split_valid_x.h5')\n",
    "valid_dir_y = os.path.join(data_dir, 'camelyonpatch_level_2_split_valid_y.h5')\n",
    "test_dir_x = os.path.join(data_dir, 'camelyonpatch_level_2_split_test_x.h5')\n",
    "\n",
    "print(train_dir_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = DataSet(train_dir_x, train_dir_y, use_percentage=1.0)\n",
    "validation_dataset = DataSet(valid_dir_x, valid_dir_y, use_percentage=1.0)\n",
    "test_dataset = DataSet(test_dir_x, use_percentage=1.0)\n",
    "print(training_dataset.get_lenght())\n",
    "training_dataset.show_image(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test batch creator\n",
    "augmentor = Augmentor()\n",
    "batch_creator = BatchCreator(validation_dataset)\n",
    "x, y = batch_creator.create_batch_sampling(5)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "    plt.rcParams['figure.figsize'] = (30,30)\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_creator.index = [0,3]\n",
    "x2, y2 = batch_creator.create_batch_sampling(5)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "    plt.rcParams['figure.figsize'] = (30,30)\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(x2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_t = Input(shape=(96, 96, 3))\n",
    "#model = createResNet50(in_t, True)\n",
    "model = CreateKaggleModel(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_params):\n",
    "    \n",
    "    batch_size = training_params['batch_size']\n",
    "    loss = training_params['loss']\n",
    "    metrics = training_params['metrics']\n",
    "    epochs = training_params['epochs']\n",
    "    steps_per_epoch = training_params['steps_per_epoch']\n",
    "    optimizer = training_params['optimizer']\n",
    "    training_dataset = training_params['training_dataset']\n",
    "    validation_dataset = training_params['validation_dataset']\n",
    "    \n",
    "    saving_dir = training_params['saving_dir']\n",
    "    model_name = training_params['model_name']\n",
    "    sampling_threshold = training_params['sampling_threshold']\n",
    "    use_sampling_strategy = training_params['use_sampling_strategy']\n",
    "    \n",
    "    log_dir = \"./logs/\"\n",
    "    if(not os.path.exists(log_dir)):\n",
    "        os.mkdir(log_dir)\n",
    "        \n",
    "    # batch generator \n",
    "    batch_creator = BatchCreator(training_dataset)\n",
    "    \n",
    "    batch_generator = batch_creator.get_generator(batch_size)\n",
    "    if use_sampling_strategy:\n",
    "        print('using sampling generator')\n",
    "        batch_generator = batch_creator.get_sampling_generator(batch_size)\n",
    "        \n",
    "    # create logger\n",
    "    logger = Logger(validation_dataset, saving_dir, model_name, batch_creator, sampling_threshold, use_sampling_strategy)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # train the model\n",
    "    model.fit_generator(generator=batch_generator, \n",
    "                        steps_per_epoch=steps_per_epoch, \n",
    "                        epochs=epochs,\n",
    "                        callbacks=[logger],\n",
    "                        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "saving_dir = './models/'\n",
    "if(not os.path.exists(saving_dir)):\n",
    "    os.mkdir(saving_dir)\n",
    "\n",
    "model_name = 'base_line_model_10_epochs'\n",
    "training_params = {}\n",
    "training_params['learning_rate'] = 0.002\n",
    "training_params['batch_size'] = 64 # number of patches in a mini-batch\n",
    "training_params['steps_per_epoch'] = int(float(training_dataset.get_lenght())/training_params['batch_size']) # number of iterations per epoch\n",
    "training_params['epochs'] = 10 # number of epochs\n",
    "\n",
    "training_params['optimizer'] = Adam(lr = training_params['learning_rate'])\n",
    "training_params['loss'] = ['binary_crossentropy']\n",
    "training_params['metrics'] = ['accuracy']\n",
    "training_params['training_dataset'] = training_dataset\n",
    "training_params['validation_dataset'] = validation_dataset\n",
    "\n",
    "training_params['saving_dir'] = saving_dir\n",
    "training_params['model_name'] = model_name\n",
    "\n",
    "training_params['use_sampling_strategy'] = False \n",
    "training_params['sampling_threshold'] = 0.75\n",
    "\n",
    "# train model\n",
    "train_model(model, training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "#model = createResNet50(in_t, True)\n",
    "model = CreateKaggleModel(True)\n",
    "model_path = os.path.join(saving_dir, model_name+'.h5')\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get traing data set\n",
    "x_test = test_dataset.get_data()\n",
    "print(test_dataset.get_lenght())\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "predictions = model.predict(x_test)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only need the second column because it holds the prediction of being cancer\n",
    "pred_cancer = predictions[:,1]\n",
    "#cases an array holding the index\n",
    "cases = np.arange(test_dataset.get_lenght())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_columns = ['case', 'prediction']\n",
    "df = pd.DataFrame(columns = pd_columns)\n",
    "df['case']       = cases\n",
    "df['prediction']       = pred_cancer\n",
    "\n",
    "submission_dir = \"./submissions/\"\n",
    "if(not os.path.exists(submission_dir)):\n",
    "    os.mkdir(submission_dir)\n",
    "\n",
    "df.to_csv(os.path.join(submission_dir,'submit_'+model_name+'.csv'), index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
