{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries\n",
    "import requests\n",
    "from tqdm import trange\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib\n",
    "from random import randint\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "from skimage.transform import rescale, resize\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate, Cropping2D, Reshape, BatchNormalization\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "import h5py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, roc_auc_score, confusion_matrix\n",
    "\n",
    "from create_model import createResNet50, CreateKaggleModel\n",
    "\n",
    "import stainNorm_Reinhard as stainNorm\n",
    "\n",
    "\n",
    "# this part is needed if you run the notebook on Cartesius with multiple cores\n",
    "#n_cores = 32\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=n_cores-1, inter_op_parallelism_threads=1, allow_soft_placement=True)\n",
    "#session = tf.Session(config=config)\n",
    "#K.set_session(session)\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = str(n_cores-1)\n",
    "#os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "#os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "#os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape = [2,3], name = 'a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape = [3,2], name = 'b')\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement = True))\n",
    "print(sess.run(c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, data_dir_x, data_dir_y = None, use_percentage = 1.0):\n",
    "        self.use_percentage = use_percentage\n",
    "        self.data_dir_x = data_dir_x\n",
    "        self.data_dir_y = data_dir_y\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        with h5py.File(self.data_dir_x, 'r') as hdf:\n",
    "            return (int)(len(hdf['x'])*self.use_percentage)\n",
    "    \n",
    "    def get_data(self):\n",
    "        with h5py.File(self.data_dir_x, 'r') as hdf:\n",
    "            return np.array( list(hdf['x']))/255\n",
    "        \n",
    "    def get_lbls(self):\n",
    "        with h5py.File(self.data_dir_y, 'r') as hdf:\n",
    "            data = list(hdf['y'])\n",
    "            return np.reshape(data,(len(data))) \n",
    "        \n",
    "    \n",
    "    def show_image(self, i):\n",
    "        with h5py.File(self.data_dir_x, 'r') as hdf:\n",
    "            data = list(hdf['x'][i])\n",
    "            img = np.array(data)\n",
    "            plt.rcParams['figure.figsize'] = (3, 3)\n",
    "            plt.imshow(img)\n",
    "            plt.title('RGB image')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentor:\n",
    "    def __init__(self, perc =.5):\n",
    "        self.perc = perc\n",
    "        \n",
    "    def rotate(self, image):\n",
    "        #rotation of 0-360\n",
    "        rows,cols,channels = image.shape\n",
    "        deg = np.random.randint(360)\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2),deg,1)\n",
    "        image = cv2.warpAffine(image,M,(cols,rows))\n",
    "        return image\n",
    "    \n",
    "    def fliphorizontal(self, image):\n",
    "        #horizontal flip\n",
    "        image = cv2.flip(image, 0)\n",
    "        return image\n",
    "    \n",
    "    def flipvertical(self, image):\n",
    "        #vertical flip\n",
    "        image = cv2.flip(image, 1)\n",
    "        return image\n",
    "    \n",
    "    def flipboth(self, image):\n",
    "        image = cv2.flip(image, -1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, dataset, augmentor = None):\n",
    "        self.dataset = dataset\n",
    "        self.dataset_length = self.dataset.get_lenght()\n",
    "        self.augmentor = augmentor\n",
    "       \n",
    "        self.index = []\n",
    "        self.batch = []\n",
    "        self.i = 0\n",
    "        \n",
    "        \n",
    "    def create_batch(self, batch_size):\n",
    "        x_data = np.array([])\n",
    "        y_data = np.array([])\n",
    "        \n",
    "        if(self.i+batch_size > self.dataset_length):\n",
    "            self.i = 0\n",
    "      \n",
    "        with h5py.File(self.dataset.data_dir_x, 'r') as hdf:\n",
    "            data = np.array(list(hdf['x'][self.i:self.i+batch_size]))/255\n",
    "                      \n",
    "            if(self.augmentor != None):\n",
    "                #augments data with random flip and rotation\n",
    "                for i in range(len(data)):\n",
    "                    data[i] = self.augmentor.rotate(data[i])\n",
    "                    augtype = np.random.randint(1,4)\n",
    "                    if(augtype == 1):\n",
    "                        data[i] = self.augmentor.fliphorizontal(data[i])\n",
    "                    elif(augtype == 2):\n",
    "                        data[i] = self.augmentor.flipvertical(data[i])\n",
    "                    elif(augtype == 3):\n",
    "                        data[i] = self.augmentor.flipboth(data[i])\n",
    "            x_data = data\n",
    "        \n",
    "        with h5py.File(self.dataset.data_dir_y, 'r') as hdf:\n",
    "            data = list(hdf['y'][self.i:self.i+batch_size])\n",
    "            data = np.reshape(data,(batch_size,1))\n",
    "            data = to_categorical(data, num_classes=2)\n",
    "            y_data = np.array(data) \n",
    "            \n",
    "        self.i += batch_size\n",
    "            \n",
    "        return x_data, y_data\n",
    "    \n",
    "\n",
    "    def create_batch_sampling(self, batch_size):\n",
    "        #init empty batch\n",
    "        x_data = np.zeros( (batch_size, 96,96,3) )\n",
    "        y_data = np.zeros( (batch_size, 2) )\n",
    "        \n",
    "        #copy the patches which have to be run again\n",
    "        for i in range(len(self.index)):\n",
    "            x_data[i] = self.batch[0][self.index[i]] \n",
    "            y_data[i] = self.batch[1][self.index[i]]\n",
    "            \n",
    "        #fill the rest of the batch with new patches\n",
    "        rest = np.arange(len(self.index), batch_size)\n",
    "        for i in rest:\n",
    "            #get random position\n",
    "            random_pos = np.random.randint(0,self.dataset_length)\n",
    "            #open x file\n",
    "            with h5py.File(self.dataset.data_dir_x, 'r') as hdf:\n",
    "                data = np.array(list(hdf['x'][random_pos]))/255\n",
    "                \n",
    "                if(self.augmentor != None):\n",
    "                    #augments data with random flip and rotation\n",
    "                    data = self.augmentor.rotate(data)\n",
    "                    augtype = np.random.randint(1,4)\n",
    "                    if(augtype == 1):\n",
    "                        data = self.augmentor.fliphorizontal(data)\n",
    "                    elif(augtype == 2):\n",
    "                        data = self.augmentor.flipvertical(data)\n",
    "                    elif(augtype == 3):\n",
    "                        data = self.augmentor.flipboth(data)\n",
    "                                                           \n",
    "                x_data[i] = data\n",
    "                \n",
    "                \n",
    "            # open y file\n",
    "            with h5py.File(self.dataset.data_dir_y, 'r') as hdf:\n",
    "                data = list(hdf['y'][i])\n",
    "                data = np.reshape(data,(1))\n",
    "                data = to_categorical(data, num_classes=2)\n",
    "                y_data[i] = data\n",
    "\n",
    "        self.batch = [x_data, y_data]\n",
    "            \n",
    "        return x_data, y_data\n",
    "    \n",
    "        \n",
    "    def get_generator(self, batch_size):\n",
    "        '''returns a generator that will yield batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_batch(batch_size)\n",
    "            \n",
    "    def get_sampling_generator(self, batch_size):\n",
    "        '''returns a generator that will yield batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_batch_sampling(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, validation_data, saving_dir, model_name, batch_creator, sampling_threshold, sampling):\n",
    "        #validation set\n",
    "        self.validation_dataset = validation_data\n",
    "        #traing batch generator\n",
    "        self.batch_creator = batch_creator\n",
    "        #threshold to for retraining a given sample\n",
    "        self.sampling_threshold = sampling_threshold\n",
    "        self.sampling = sampling\n",
    "        self.time = time.time()\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        #get the validation data set\n",
    "        self.val_imgs = self.validation_dataset.get_data()\n",
    "        self.val_lbls = self.validation_dataset.get_lbls()\n",
    "        \n",
    "        self.Sensitivity = []\n",
    "        self.Specificity = []\n",
    "        self.auc = 0\n",
    "        self.aucs = []\n",
    "        \n",
    "        self.model_filename = os.path.join(saving_dir, model_name + '.h5')\n",
    "        \n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.best_validation_acc = 0\n",
    "        self.best_model = None\n",
    "        self.predictions = None\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if self.sampling:\n",
    "            \n",
    "            y_pred = self.model.predict(self.batch_creator.batch[0])\n",
    "            y_true = self.batch_creator.batch[1][:,1]\n",
    "            y_pred_argmax = np.argmax(y_pred, axis = 1)\n",
    "            y_pred_max = np.max(y_pred, axis = 1)\n",
    "\n",
    "            index = []\n",
    "            for i in range(len(y_true)):\n",
    "                # if the prediction is wrong add the retrain the sample\n",
    "                if y_true[i] != y_pred_argmax[i]:\n",
    "                    index.append(i)\n",
    "                #else if the prediction is correct but below the threshold retrain the sample\n",
    "                elif y_pred_max[i] < self.sampling_threshold:\n",
    "                    index.append(i)\n",
    "            self.batch_creator.index = index\n",
    "    \n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        acc = self.validate()\n",
    "        self.accuracies.append([len(self.losses), acc])\n",
    "        if acc > self.best_validation_acc:\n",
    "            self.best_validation_acc = acc\n",
    "            self.model.save(self.model_filename) # save best model to disk\n",
    "            print('best model saved as {}'.format(self.model_filename))\n",
    "        self.plot()   \n",
    "        \n",
    "        t = time.time() - self.time\n",
    "        hours = int(t/3600)\n",
    "        t = t - hours*3600\n",
    "        minutes = int(t/60)\n",
    "        t = t - minutes*60\n",
    "        seconds = t\n",
    "    \n",
    "        #print to logger file\n",
    "        with open(\"./logs/current_status_{}.txt\".format(self.model_name), \"a\") as myfile:\n",
    "            myfile.write(str(epoch) + ' | ' + str(logs) + ' | val_acc = ' + str(acc)+' | auc = '+str(self.auc) \n",
    "                         +' | time = '+str(hours)+':'+str(minutes)+':'+str(seconds)+'\\n')\n",
    "        \n",
    "    \n",
    "    def validate(self):\n",
    "        #predict validation set\n",
    "        pred_lbls = self.model.predict(self.val_imgs)\n",
    "        predicted_lbls = np.argmax(pred_lbls, axis=1)\n",
    "        \n",
    "        self.Specificity, self.Sensitivity, thresholds = roc_curve(self.val_lbls, pred_lbls[:,1])\n",
    "        self.auc = roc_auc_score(self.val_lbls, pred_lbls[:,1])\n",
    "        self.aucs.append(self.auc)\n",
    "                    \n",
    "        return compute_accuracy(self.val_lbls, predicted_lbls)\n",
    "\n",
    "    \n",
    "    def plot(self):\n",
    "        clear_output()\n",
    "        N = len(self.losses)\n",
    "        plt.figure(figsize=(50, 10))\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.plot(range(0, N), self.losses); plt.title('losses')\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.plot(np.array(self.accuracies)); plt.title('accuracies')\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.plot(np.array(self.aucs)); plt.title('AUC')\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.plot(np.array(self.Specificity), np.array(self.Sensitivity)); \n",
    "        plt.title(' AUC = {}'.format(self.auc));\n",
    "        plt.xlabel('1 - Specificity');\n",
    "        plt.ylabel('Sensitivity')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/camelyonpatch_level_2_split_train_x.h5\n"
     ]
    }
   ],
   "source": [
    "#data dirs\n",
    "data_dir = './data/'\n",
    "data_dir_norm = './data/normalized/'\n",
    "\n",
    "#train_dir_x = os.path.join(data_dir_norm, 'x_train.h5')\n",
    "#train_dir_y = os.path.join(data_dir, 'camelyonpatch_level_2_split_train_y.h5')\n",
    "#valid_dir_x = os.path.join(data_dir_norm, 'x_valid.h5')\n",
    "#valid_dir_y = os.path.join(data_dir, 'camelyonpatch_level_2_split_valid_y.h5')\n",
    "#test_dir_x = os.path.join(data_dir_norm, 'x_test.h5')\n",
    "\n",
    "train_dir_x = os.path.join(data_dir, 'camelyonpatch_level_2_split_train_x.h5')\n",
    "train_dir_y = os.path.join(data_dir, 'camelyonpatch_level_2_split_train_y.h5')\n",
    "valid_dir_x = os.path.join(data_dir, 'camelyonpatch_level_2_split_valid_x.h5')\n",
    "valid_dir_y = os.path.join(data_dir, 'camelyonpatch_level_2_split_valid_y.h5')\n",
    "test_dir_x = os.path.join(data_dir, 'camelyonpatch_level_2_split_test_x.h5')\n",
    "\n",
    "print(train_dir_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 94, 94, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 92, 92, 32)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 92, 92, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 92, 92, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 46, 46, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 46, 46, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 44, 44, 64)        18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 42, 42, 64)        36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 42, 42, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 21, 21, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21, 21, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 19, 19, 128)       73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 19, 19, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 17, 17, 128)       147456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 17, 17, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097152   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,386,946\n",
      "Trainable params: 2,385,602\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "saving_dir = './models/'\n",
    "model_name = \"model_aug_15_epochs\"\n",
    "#model = createResNet50(in_t, True)\n",
    "model = CreateKaggleModel(True)\n",
    "model_path = os.path.join(saving_dir, model_name+'.h5')\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "4096.0\n"
     ]
    }
   ],
   "source": [
    "training_dataset = DataSet(train_dir_x, train_dir_y, use_percentage=1.0)\n",
    "\n",
    "batch_size = 64\n",
    "n_steps = int(float(training_dataset.get_lenght())/batch_size)\n",
    "print(n_steps)\n",
    "print(float(training_dataset.get_lenght())/batch_size)\n",
    "\n",
    "batch_creator = BatchCreator(training_dataset)\n",
    "    \n",
    "batch_generator = batch_creator.get_generator(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096/4096 [==============================] - 218s 53ms/step\n",
      "(262144, 2)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(batch_generator, steps = n_steps, verbose = 1)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 0 1]\n",
      "(262144,)\n"
     ]
    }
   ],
   "source": [
    "lbls = training_dataset.get_lbls()\n",
    "print(lbls)\n",
    "print(lbls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 0 1]\n",
      "(262144,)\n"
     ]
    }
   ],
   "source": [
    "predicted_lbl = np.argmax(predictions, axis = 1)\n",
    "print(predicted_lbl)\n",
    "print(predicted_lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235211\n",
      "26933\n"
     ]
    }
   ],
   "source": [
    "learned = []\n",
    "not_learned = []\n",
    "for i in range(len(predicted_lbl)):\n",
    "    if predicted_lbl[i] == lbls[i]:\n",
    "        learned.append(i)\n",
    "    else:\n",
    "        not_learned.append(i)\n",
    "\n",
    "print(len(learned))\n",
    "print(len(not_learned))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_columns = ['learned']\n",
    "df = pd.DataFrame(columns = pd_columns)\n",
    "df['learned']       = learned\n",
    "\n",
    "\n",
    "data_dir = \"./data/\"\n",
    "if(not os.path.exists(data_dir)):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "df.to_csv(os.path.join(data_dir,'learned.csv'), index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_columns = ['not_learned']\n",
    "df = pd.DataFrame(columns = pd_columns)\n",
    "df['not_learned']       = not_learned\n",
    "\n",
    "\n",
    "data_dir = \"./data/\"\n",
    "if(not os.path.exists(data_dir)):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "df.to_csv(os.path.join(data_dir,'not_learned.csv'), index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
